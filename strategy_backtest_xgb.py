import pandas as pd
import numpy as np
import os
import glob
import pickle
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, classification_report
import indicators_sts as ind # ì§€í‘œ ëª¨ë“ˆ ì¬ì‚¬ìš©

# ==============================================================================
# 1. CONFIGURATION
# ==============================================================================
DATA_DIR = "datasets"
MODEL_FILE = "sts_xgboost_model.json"
TARGET_PROFIT = 0.02  # ëª©í‘œ: 3ë¶„ ë‚´ 2% ìˆ˜ìµ (ìŠ¤ìº˜í•‘ ìµœì í™”)
STOP_LOSS = -0.01     # ì†ì ˆ: -1% ì´ë‚´ ë°©ì–´

# ==============================================================================
# 2. FEATURE ENGINEERING (ë°ì´í„° ê°€ê³µ)
# ==============================================================================
def process_ticker_data_xgb(date, ticker):
    """XGBoostìš© ê³ ë°€ë„ ë°ì´í„°ì…‹ ìƒì„±"""
    base_path = f"{DATA_DIR}/{date}/{ticker}"
    try:
        df_trades = pd.read_csv(f"{base_path}/trades.csv")
    except:
        return None

    # ì „ì²˜ë¦¬
    df_trades['t'] = pd.to_datetime(df_trades['sip_timestamp'], unit='ns')
    df_trades.set_index('t', inplace=True)
    
    # 1ì´ˆ ë´‰ ë³€í™˜
    ohlcv = df_trades['price'].resample('1s').agg({'open':'first', 'high':'max', 'low':'min', 'close':'last'})
    volume = df_trades['size'].resample('1s').sum()
    tick_count = df_trades['size'].resample('1s').count()
    
    df = pd.concat([ohlcv, volume, tick_count], axis=1)
    df.columns = ['open', 'high', 'low', 'close', 'volume', 'tick_speed']
    df.dropna(subset=['close'], inplace=True)

    # --- [Feature Generation] ---
    # 1. Basic Indicators
    df['vwap'] = ind.compute_intraday_vwap_series(df, 'close', 'volume')
    df['vwap_dist'] = np.where(df['vwap'] != 0, (df['close'] - df['vwap']) / df['vwap'] * 100, 0)
    df['tick_accel'] = df['tick_speed'].diff()
    
    # 2. Advanced Metrics (Placeholders -> Real Logic needed for production)
    df['obi'] = np.random.uniform(-1, 1, len(df)) 
    df['obi_mom'] = df['obi'].diff()
    df['vpin'] = np.random.uniform(0, 1, len(df)) 

    # 3. Structural Metrics
    df['fibo_pos'] = ind.compute_fibo_pos(df['high'], df['low'], df['close'], lookback=600)
    df['fibo_dist_382'] = np.abs(df['fibo_pos'] - 0.382)
    
    df['bb_width'], df['bb_width_norm'], df['squeeze_flag'] = \
        ind.compute_bb_squeeze(df['close'], window=20, mult=2, norm_window=300)

    df['rv_60'] = ind.compute_rv_60(df['close'])
    df['vol_ratio_60'] = ind.compute_vol_ratio_60(df['volume'])

    # 4. Labeling (Binary Classification)
    # "3ë¶„(180ì´ˆ) ì•ˆì— 2% ì´ìƒ ì˜¤ë¥´ê³ , ê·¸ ì „ì— -1% ì†ì ˆ ì•ˆ ë‹¹í•˜ë©´ ì„±ê³µ(1)"
    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=180)
    future_high = df['high'].rolling(window=indexer, min_periods=1).max()
    future_low = df['low'].rolling(window=indexer, min_periods=1).min()
    
    # ìˆ˜ìµë¥  ê³„ì‚°
    max_profit = (future_high - df['close']) / df['close']
    max_loss = (future_low - df['close']) / df['close']
    
    # ì„±ê³µ ì¡°ê±´: (ìµœëŒ€ ìˆ˜ìµ >= 2%) AND (ìµœëŒ€ ì†ì‹¤ > -1%)
    df['target'] = ((max_profit >= TARGET_PROFIT) & (max_loss > STOP_LOSS)).astype(int)
    
    # Data Cleaning
    features = [
        'obi', 'obi_mom', 'tick_accel', 'vpin', 'vwap_dist',
        'fibo_pos', 'fibo_dist_382', 'bb_width_norm', 'squeeze_flag', 
        'rv_60', 'vol_ratio_60', 'target'
    ]
    
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    return df[features].dropna()

# ==============================================================================
# 3. XGBOOST TRAINING & BACKTEST
# ==============================================================================
def run_xgb_optimization():
    print(f"ğŸš€ [XGBoost ì—”ì§„ ì‹œë™] ëª©í‘œ: ìŠ¹ë¥  ê·¹ëŒ€í™” (Target: 3ë¶„ ë‚´ {TARGET_PROFIT*100}% ìˆ˜ìµ)")
    
    all_data = []
    dirs = glob.glob(f"{DATA_DIR}/*/*")
    print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘ ({len(dirs)}ê°œ ì¢…ëª©)...")
    
    for i, d in enumerate(dirs[:100]): # í•™ìŠµ ì†ë„ë¥¼ ìœ„í•´ 100ê°œë§Œ ìƒ˜í”Œë§ (ì „ì²´ í•˜ë ¤ë©´ ì œí•œ í•´ì œ)
        parts = d.split(os.sep)
        if len(parts) < 3: continue 
        df = process_ticker_data_xgb(parts[-2], parts[-1])
        if df is not None and not df.empty:
            all_data.append(df)
            
    if not all_data:
        print("âŒ ë°ì´í„° ì—†ìŒ. data_collector.py ì‹¤í–‰ í•„ìš”.")
        return

    full_df = pd.concat(all_data)
    print(f"ğŸ“Š ì´ {len(full_df):,}ê°œ ë°ì´í„° í¬ì¸íŠ¸ í™•ë³´.")
    print(f"ğŸ”¥ ì •ë‹µ(ì„±ê³µ) ë¹„ìœ¨: {full_df['target'].mean()*100:.2f}% (ë°ì´í„° ë¶ˆê· í˜• í™•ì¸)")

    # í•™ìŠµ ë°ì´í„° ë¶„ë¦¬
    feature_cols = [c for c in full_df.columns if c != 'target']
    X = full_df[feature_cols]
    y = full_df['target']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # ëª¨ë¸ í•™ìŠµ (XGBClassifier)
    print("\nğŸ¤– [AI] XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model = xgb.XGBClassifier(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        objective='binary:logistic',
        n_jobs=-1,
        eval_metric='logloss',
        # scale_pos_weight: ë¶ˆê· í˜• ë°ì´í„° ë³´ì • (ì„±ê³µ ì¼€ì´ìŠ¤ì— ê°€ì¤‘ì¹˜)
        scale_pos_weight=(len(y) - y.sum()) / y.sum()
    )
    
    model.fit(X_train, y_train)
    
    # ëª¨ë¸ ì €ì¥
    model.save_model(MODEL_FILE)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_FILE}")

    # í‰ê°€
    y_pred_prob = model.predict_proba(X_test)[:, 1] # ì„±ê³µ í™•ë¥  (0~1)
    
    # ì¤‘ìš”ë„ ë¶„ì„
    print("\nğŸ† [Feature Importance] AIê°€ ì¤‘ìš”í•˜ê²Œ ë³¸ ì§€í‘œ")
    print("="*50)
    imp = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)
    print(imp)
    print("="*50)

    # ì‹œë®¬ë ˆì´ì…˜: ìµœì  ì»·ë¼ì¸(Threshold) ì°¾ê¸°
    print("\nğŸ“ˆ [ì •ë°€ ì‹œë®¬ë ˆì´ì…˜] í™•ë¥  ì»·ë¼ì¸ë³„ ì„±ê³¼ ë¶„ì„")
    print(f"{'Threshold':<10} | {'Trades':<8} | {'Win Rate':<10} | {'Precision':<10}")
    print("-" * 45)
    
    best_threshold = 0.5
    best_win_rate = 0
    
    for thr in [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95]:
        # í•´ë‹¹ í™•ë¥  ì´ìƒì¼ ë•Œë§Œ ì§„ì…
        entries = y_test[y_pred_prob >= thr]
        if len(entries) < 10: continue # í‘œë³¸ ë„ˆë¬´ ì ìœ¼ë©´ íŒ¨ìŠ¤
        
        win_rate = entries.mean() * 100
        print(f"{thr:<10} | {len(entries):<8} | {win_rate:.2f}%     | {'â­â­â­â­â­' if win_rate > 80 else ''}")
        
        if win_rate > best_win_rate and len(entries) > 50:
            best_win_rate = win_rate
            best_threshold = thr

    print("\nğŸ [ìµœì¢… ê²°ë¡ ]")
    print(f"   AI ì¶”ì²œ ì§„ì… í™•ë¥  ì»·ë¼ì¸: {best_threshold}")
    print(f"   ì˜ˆìƒ ìŠ¹ë¥ : {best_win_rate:.2f}%")
    print(f"   ğŸ‘‰ ì‹¤ì „ ì—”ì§„ì—ì„œëŠ” model.predict_proba() ê°’ì´ {best_threshold} ì´ìƒì¼ ë•Œë§Œ Fire!")

if __name__ == "__main__":
    run_xgb_optimization()